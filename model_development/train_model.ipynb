{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a2260a",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for training model\n",
    "\n",
    "Note: use smaller learning_rate to prevent huge changes (i.e. forgetting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_folder = \"./t5_small_finetuned\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_folder)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4cbdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"cnn_dailymail_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705d544a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c296da5d5d2426e92cf8f7b9f126243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b82f1f13ac4931a94f6901f1618925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take subsets of dataset\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(8000))\n",
    "val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(1000))\n",
    "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# preprocess datasets\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess(batch):\n",
    "    inputs = [\"summarize: \" + article for article in batch[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(batch[\"highlights\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5_small_finetuned_training\",  # save model in dir\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    eval_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training from last checkpoint if interrupted\n",
    "trainer.train(resume_from_checkpoint=\"./t5_small_finetuned_training/checkpoint-500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d5b1ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./t5_small_finetuned\\\\tokenizer_config.json',\n",
       " './t5_small_finetuned\\\\special_tokens_map.json',\n",
       " './t5_small_finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model from checkpoint\n",
    "checkpoint_folder = \"./t5_efficient_mini_finetuned_training/checkpoint-100\"\n",
    "# model_test = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_folder)\n",
    "# tokenizer_test = AutoTokenizer.from_pretrained(checkpoint_folder)\n",
    "\n",
    "# save final model (which can then be loaded as in tutorial) (adapt before execution!)\n",
    "model.save_pretrained(\"./t5_small_finetuned\")\n",
    "tokenizer.save_pretrained(\"./t5_small_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96e810",
   "metadata": {},
   "source": [
    "# Tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d438d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:\n",
      "The mayor of the city announced new environmental policies today aimed at reducing emissions \n",
      "    by 40% by 2035. The initiative includes expanded public transit, incentives for electric vehicles, and stricter \n",
      "    regulations on industrial polluters.\n",
      "Generated Summary:\n",
      "The initiative includes expanded public transit, incentives for electric vehicles and stricter regulations on industrial polluters.\n",
      "Reference Summary:\n",
      "The presidential hopeful held a town hall meeting in Kenilworth on Tuesday .\n",
      "During the meeting, high school English teacher Kathy Mooney got up to ask the governor a question about pensions .\n",
      "She asked why he didn't seek a higher legal settlement in a case with ExxonMobil that would have contributed to the state's pension system .\n",
      "Christie responded by repeatedly asking how much Mooney knew about the deal instead of answering her question .\n"
     ]
    }
   ],
   "source": [
    "# get output of model\n",
    "article = \"\"\"The mayor of the city announced new environmental policies today aimed at reducing emissions \n",
    "    by 40% by 2035. The initiative includes expanded public transit, incentives for electric vehicles, and stricter \n",
    "    regulations on industrial polluters.\"\"\"\n",
    "article = test_dataset[2][\"article\"]\n",
    "print(\"Article:\\n\" + article)\n",
    "\n",
    "input_text = \"summarize: \" + article  # article with task prefix\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"], max_length=128, num_beams=4, length_penalty=1.0, no_repeat_ngram_size=2, early_stopping=True\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\\n\" + summary)\n",
    "print(\"Reference Summary:\\n\" + test_dataset[2][\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and saving models\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./t5_efficient_mini\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./t5_efficient_mini\")\n",
    "\n",
    "# save model\n",
    "# model_raw.save_pretrained(\"./t5_efficient_mini\")\n",
    "# tokenizer_raw.save_pretrained(\"./t5_efficient_mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14134517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and saving datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# load dataset\n",
    "dataset = load_from_disk(\"cnn_dailymail_full\")\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# save dataset (no additional import needed)\n",
    "# dataset.save_to_disk(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2814d03",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705de4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished successfully\n",
    "# download cnn_dailymail dataset from internet and save it locally\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "# dataset.save_to_disk(\"cnn_dailymail_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27222cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished successfully\n",
    "# download raw T5-efficient-mini from internet and save it locally\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"google/t5-efficient-mini\"\n",
    "model_raw = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer_raw = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model_raw.save_pretrained(\"./t5_efficient_mini\")\n",
    "# tokenizer_raw.save_pretrained(\"./t5_efficient_mini\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
